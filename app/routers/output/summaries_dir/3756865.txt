grover 's algorithm for quantum searching of a database is generalized to deal with arbitrary initial amplitude distributions . first order linear difference equations are found for the time evolution of the amplitudes of the r marked and n - r unmarked states . these equations are solved exactly . an expression for the optimal measurement time t \sim o(\sqrt{n / r } ) is derived which is shown to depend only on the initial average amplitudes of the marked and unmarked states . a bound on the probability of measuring a marked state is derived , which depends only on the standard deviation of the initial amplitude distributions of the marked or unmarked states .