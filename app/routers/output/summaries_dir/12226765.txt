bosonsampling is an intermediate model of quantum computation where linear - optical networks are used to solve sampling problems expected to be hard for classical computers . since these devices are not expected to be universal for quantum computation , it remains an open question of whether any error - correction techniques can be applied to them , and thus it is important to investigate how robust the model is under natural experimental imperfections , such as losses and imperfect control of parameters . here we investigate the complexity of bosonsampling under photon losses --- more specifically , the case where an unknown subset of the photons are randomly lost at the sources . we show that , if $ k$ out of $ n$ photons are lost , then we can not sample classically from a distribution that is $ 1 / n^{\theta(k)}$-close ( in total variation distance ) to the ideal distribution , unless a $ \text{bpp}^{\text{np}}$ machine can estimate the permanents of gaussian matrices in $ n^{o(k)}$ time . in particular , if $ k$ is constant , this implies that simulating lossy bosonsampling is hard for a classical computer , under exactly the same complexity assumption used for the original lossless case .