gaussian boson sampling is a model of photonic quantum computing where single - mode squeezed states are sent through linear - optical interferometers and measured using single - photon detectors . in this work , we employ a recent exact sampling algorithm for gbs with threshold detectors to perform classical simulations on the titan supercomputer . we determine the time and memory resources as well as the amount of computational nodes required to produce samples for different numbers of modes and detector clicks . it is possible to simulate a system with 800 optical modes postselected on outputs with 20 detector clicks , producing a single sample in roughly two hours using $ 40\%$ of the available nodes of titan . additionally , we benchmark the performance of gbs when applied to dense subgraph identification , even in the presence of photon loss . we perform sampling for several graphs containing as many as 200 vertices . our findings indicate that large losses can be tolerated and that the use of threshold detectors is preferable over using photon - number - resolving detectors postselected on collision - free outputs .