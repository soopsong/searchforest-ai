reservoir computing ( rc ) is a novel approach to time series prediction using recurrent neural networks . in rc , an input signal perturbs the intrinsic dynamics of a medium called a reservoir . a readout layer is then trained to reconstruct a target output from the reservoir 's state . the multitude of rc architectures and evaluation metrics poses a challenge to both practitioners and theorists who study the task - solving performance and computational power of rc . in addition , in contrast to traditional computation models , the reservoir is a dynamical system in which computation and memory are inseparable , and therefore hard to analyze . here , we compare echo state networks ( esn ) , a popular rc architecture , with tapped - delay lines ( dl ) and nonlinear autoregressive exogenous ( narx ) networks , which we use to model systems with limited computation and limited memory respectively . we compare the performance of the three systems while computing three common benchmark time series : h{\'e}non map , narma10 , and narma20 . we find that the role of the reservoir in the reservoir computing paradigm goes beyond providing a memory of the past inputs . the dl and the narx network have higher memorization capability , but fall short of the generalization power of the esn .