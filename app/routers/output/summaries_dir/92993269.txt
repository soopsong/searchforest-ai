ridge regression ( rr ) is an important machine learning technique which introduces a regularization hyperparameter $ \alpha$ to ordinary multiple linear regression for analyzing data suffering from multicollinearity . in this paper , we present a quantum algorithm for rr , where the technique of parallel hamiltonian simulation to simulate a number of hermitian matrices in parallel is proposed and used to develop a quantum version of $ k$-fold cross - validation approach , which can efficiently estimate the predictive performance of rr . our algorithm consists of two phases : ( 1 ) using quantum $ k$-fold cross - validation to efficiently determine a good $ \alpha$ with which rr can achieve good predictive performance , and then ( 2 ) generating a quantum state encoding the optimal fitting parameters of rr with such $ \alpha$ , which can be further utilized to predict new data . since indefinite dense hamiltonian simulation has been adopted as a key subroutine , our algorithm can efficiently handle non - sparse data matrices . it is shown that our algorithm can achieve exponential speedup over the classical counterpart for ( low - rank ) data matrices with low condition numbers . but when the condition numbers of data matrices is large to be amenable to full or approximately full ranks of data matrices , only polynomial speedup can be achieved .