restricted boltzmann machines ( rbms ) and their extensions , called ' deep - belief networks ' , are powerful neural networks that have found applications in the fields of machine learning and artificial intelligence . the standard way to training these models resorts to an iterative unsupervised procedure based on gibbs sampling , called ' contrastive divergence ' ( cd ) , and additional supervised tuning via back - propagation . however , this procedure has been shown not to follow any gradient and can lead to suboptimal solutions . in this paper , we show an efficient alternative to cd by means of simulations of digital memcomputing machines ( dmms ) . we test our approach on pattern recognition using a modified version of the mnist data set . dmms sample effectively the vast phase space given by the model distribution of the rbm , and provide a very good approximation close to the optimum . this efficient search significantly reduces the number of pretraining iterations necessary to achieve a given level of accuracy , as well as a total performance gain over cd . in fact , the acceleration of pretraining achieved by simulating dmms is comparable to , in number of iterations , the recently reported hardware application of the quantum annealing method on the same network and data set . notably , however , dmms perform far better than the reported quantum annealing results in terms of quality of the training . we also compare our method to advances in supervised training , like batch - normalization and rectifiers , that work to reduce the advantage of pretraining . we find that the memcomputing method still maintains a quality advantage ( $ > 1\%$ in accuracy , and a $ 20\%$ reduction in error rate ) over these approaches . furthermore , our method is agnostic about the connectivity of the network . therefore , it can be extended to train full boltzmann machines , and even deep networks at once .