nonlinear photonic delay systems present interesting implementation platforms for machine learning models . they can be extremely fast , offer great degrees of parallelism and potentially consume far less power than digital processors . so far they have been successfully employed for signal processing using the reservoir computing paradigm . in this paper we show that their range of applicability can be greatly extended if we use gradient descent with backpropagation through time on a model of the system to optimize the input encoding of such systems . we perform physical experiments that demonstrate that the obtained input encodings work well in reality , and we show that optimized systems perform significantly better than the common reservoir computing approach . the results presented here demonstrate that common gradient descent techniques from machine learning may well be applicable on physical neuro - inspired analog computers .