squeezed states of light have been successfully employed in interferometric gravitational - wave detectors to reduce quantum noise , thus becoming one of the most promising options for extending the astrophysical reach of the generation of detectors currently under construction worldwide . in these advanced instruments , quantum noise will limit sensitivity over the entire detection band . therefore , to obtain the greatest benefit from squeezing , the injected squeezed state must be filtered using a long - storage - time optical resonator , or"filter cavity " , so as to realise a frequency dependent rotation of the squeezed quadrature . whilst the ultimate performance of a filter cavity is determined by its storage time , several practical decoherence and degradation mechanisms limit the experimentally achievable quantum noise reduction . in this paper we develop an analytical model to explore these mechanisms in detail . as an example , we apply our results to the 16 m filter cavity design currently under consideration for the advanced ligo interferometers .