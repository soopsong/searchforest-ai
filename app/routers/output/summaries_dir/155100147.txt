we propose a method for learning a quantum probabilistic model of a perceptron . by considering a cross entropy between two density matrices we can learn a model that takes noisy output labels into account while learning . a multitude of proposals already exist that aim to utilize the curious properties of quantum systems to build a quantum perceptron , but these proposals rely on a classical cost function for the optimization procedure . we demonstrate the usage of a quantum equivalent of the classical log - likelihood , which allows for a quantum model and training procedure . we show that this allows us to better capture noisyness in data compared to a classical perceptron . by considering entangled qubits we can learn nonlinear separation boundaries , such as xor .