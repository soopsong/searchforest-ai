machine learning techniques have led to broad adoption of a statistical model of computing . the statistical distributions natively available on quantum processors are a superset of those available classically . harnessing this attribute has the potential to accelerate or otherwise improve machine learning relative to purely classical performance . a key challenge toward that goal is learning to hybridize classical computing resources and traditional learning techniques with the emerging capabilities of general purpose quantum processors . here , we demonstrate such hybridization by training a 19-qubit gate model processor to solve a clustering problem , a foundational challenge in unsupervised learning . we use the quantum approximate optimization algorithm in conjunction with a gradient - free bayesian optimization to train the quantum machine . this quantum / classical hybrid algorithm shows robustness to realistic noise , and we find evidence that classical optimization can be used to train around both coherent and incoherent imperfections .